{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using notTheano backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "You need the TensorFlow (v1) module installed to use TensorBoard.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a60dd0de9314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-a60dd0de9314>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     train(data_type, seq_length, model, saved_model=saved_model,\n\u001b[1;32m    112\u001b[0m           \u001b[0mclass_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m           load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a60dd0de9314>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_type, seq_length, model, saved_model, class_limit, image_shape, load_to_memory, batch_size, nb_epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Helper: TensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ml/Documents/CNN/data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Helper: Stop when we stop learning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/callbacks/tensorboard_v1.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, histogram_freq, batch_size, write_graph, write_grads, write_images, embeddings_freq, embeddings_layer_names, embeddings_metadata, embeddings_data, update_freq)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ImportError('You need the TensorFlow (v1) module installed to '\n\u001b[0m\u001b[1;32m     99\u001b[0m                               'use TensorBoard.')\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: You need the TensorFlow (v1) module installed to use TensorBoard."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train our RNN on extracted features or images.\n",
    "\"\"\"\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from models import ResearchModels\n",
    "from data import DataSet\n",
    "import time\n",
    "import os.path\n",
    "import os\n",
    "import data\n",
    "\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def train(data_type, seq_length, model, saved_model=None,\n",
    "          class_limit=None, image_shape=None,\n",
    "          load_to_memory=False, batch_size=32, nb_epoch=100):\n",
    "    # Helper: Save the model.\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath=os.path.join('/home/ml/Documents/CNN/data', 'checkpoints', model + '-' + data_type + \\\n",
    "            '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "\n",
    "    # Helper: TensorBoard\n",
    "    tb = TensorBoard(log_dir=os.path.join('/home/ml/Documents/CNN/data', 'logs', model))\n",
    "\n",
    "    # Helper: Stop when we stop learning.\n",
    "    early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "    # Helper: Save results.\n",
    "    timestamp = time.time()\n",
    "    csv_logger = CSVLogger(os.path.join('/home/ml/Documents/CNN/data', 'logs', model + '-' + 'training-' + \\\n",
    "        str(timestamp) + '.log'))\n",
    "\n",
    "    # Get the data and process it.\n",
    "    if image_shape is None:\n",
    "        data = DataSet(\n",
    "            seq_length=seq_length,\n",
    "            class_limit=class_limit\n",
    "        )\n",
    "    else:\n",
    "        data = DataSet(\n",
    "            seq_length=seq_length,\n",
    "            class_limit=class_limit,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "    # Get samples per epoch.\n",
    "    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
    "    steps_per_epoch = (len(data.data) * 0.7) // batch_size\n",
    "\n",
    "    if load_to_memory:\n",
    "        # Get data.\n",
    "        X, y = data.get_all_sequences_in_memory('train', data_type)\n",
    "        X_test, y_test = data.get_all_sequences_in_memory('test', data_type)\n",
    "    else:\n",
    "        # Get generators.\n",
    "        generator = data.frame_generator(batch_size, 'train', data_type)\n",
    "        val_generator = data.frame_generator(batch_size, 'test', data_type)\n",
    "\n",
    "    # Get the model.\n",
    "    rm = ResearchModels(len(data.classes), model, seq_length, saved_model)\n",
    "\n",
    "    # Fit!\n",
    "    if load_to_memory:\n",
    "        # Use standard fit.\n",
    "        rm.model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=1,\n",
    "            callbacks=[tb, early_stopper, csv_logger],\n",
    "            epochs=nb_epoch)\n",
    "    else:\n",
    "        # Use fit generator.\n",
    "        rm.model.fit_generator(\n",
    "            generator=generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=nb_epoch,\n",
    "            verbose=1,\n",
    "            callbacks=[tb, early_stopper, csv_logger, checkpointer],\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=40,\n",
    "            workers=4)\n",
    "\n",
    "def main():\n",
    "    \"\"\"These are the main training settings. Set each before running\n",
    "    this file.\"\"\"\n",
    "    # model can be one of lstm, lrcn, mlp, conv_3d, c3d\n",
    "    model = \"lstm\"\n",
    "    saved_model = None  # None or weights file\n",
    "    class_limit = 1  # int, can be 1-101 or None\n",
    "    seq_length =40\n",
    "    load_to_memory = False  # pre-load the sequences into memory\n",
    "    batch_size = 32\n",
    "    nb_epoch = 100\n",
    "\n",
    "    # Chose images or features and image shape based on network.\n",
    "    if model in ['conv_3d', 'c3d', 'lrcn']:\n",
    "        data_type = 'images'\n",
    "        image_shape = (80, 80, 3)\n",
    "    elif model in ['lstm', 'mlp']:\n",
    "        data_type = 'features'\n",
    "        image_shape = None\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model. See train.py for options.\")\n",
    "\n",
    "    train(data_type, seq_length, model, saved_model=saved_model,\n",
    "          class_limit=class_limit, image_shape=image_shape,\n",
    "          load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
